{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glass\n",
    "import backbones\n",
    "import utils\n",
    "from perlin import perlin_mask\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(description=\"glass\")\n",
    "parser.add_argument('--image_size', default=288, type=int) # 288\n",
    "parser.add_argument('--resize', default=288, type=int) # 288\n",
    "parser.add_argument('--backbone', default='wideresnet50', type=str) \n",
    "parser.add_argument('--layers_to_extract_from', nargs='+', default=['layer2', 'layer3'], type=str) # 연산량에 큰 영향을 주는 파라미터 (성능만 괜찮으면 layer를 일부 제거해도 무방)\n",
    "parser.add_argument('--pretrain_embed_dimension', default=1024, type=int)\n",
    "parser.add_argument('--target_embed_dimension', default=1024, type=int)\n",
    "parser.add_argument('--patchsize', default=3, type=int)\n",
    "parser.add_argument('--coreset_rate', default=0.1, type=float) # 연산량에 큰 영향을 주는 파라미터 (0.01 - 0.25)\n",
    "parser.add_argument('--anomaly_scorer_num_nn', default=1, type=int)\n",
    "parser.add_argument('--batch_size', default=8, type=int)\n",
    "parser.add_argument('--batch_size_inf', default=8, type=int) # 실제 capacitor cap 데이터의 열에 일치하도록 임의 설정\n",
    "parser.add_argument('--augment', default=True, type=bool)\n",
    "parser.add_argument('--seed', default=0, type=int) \n",
    "parser.add_argument('--device', nargs='+', default=[0], type=int)\n",
    "parser.add_argument('--num_workers', default=0, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "image_size = args.image_size\n",
    "resize = args.resize\n",
    "BATCH_SIZE = args.batch_size\n",
    "BATCH_SIZE_INF = args.batch_size_inf\n",
    "SEED = args.seed\n",
    "\n",
    "# create save path\n",
    "save_root = \"saved\"\n",
    "if not os.path.exists(save_root):\n",
    "    os.mkdir(save_root)\n",
    "\n",
    "# set random seeds\n",
    "def set_seeds(seed=SEED):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    pl.seed_everything(SEED)\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class DatasetSplit(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    TEST = \"test\"\n",
    "\n",
    "class AnomalyDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 source=\"D:/Dev/Data/mvtec/speefox\",\n",
    "                 anomaly_source_path=\"D:/Dev/Data/dtd/images\",\n",
    "                 resize=288,\n",
    "                 imagesize=288,\n",
    "                 split=DatasetSplit.TRAIN,\n",
    "                 rotate_degrees=0,\n",
    "                 translate=0,\n",
    "                 brightness_factor=0,\n",
    "                 contrast_factor=0,\n",
    "                 saturation_factor=0,\n",
    "                 gray_p=0,\n",
    "                 h_flip_p=0,\n",
    "                 v_flip_p=0,\n",
    "                 distribution=0,\n",
    "                 mean=0.5,\n",
    "                 std=0.1,\n",
    "                 fg=0,\n",
    "                 rand_aug=1,\n",
    "                 downsampling=8,\n",
    "                 scale=0,\n",
    "                 anomaly_type=None,):\n",
    "        super().__init__()\n",
    "        self.source = source\n",
    "        self.split = split\n",
    "        self.distribution = distribution\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.fg = fg\n",
    "        self.rand_aug = rand_aug\n",
    "        self.downsampling = downsampling\n",
    "        self.resize = resize if self.distribution != 1 else [resize, resize]\n",
    "        self.imgsize = imagesize\n",
    "        self.imagesize = (3, self.imgsize, self.imgsize)\n",
    "        self.anomaly_type = anomaly_type\n",
    "\n",
    "        self.imgpaths, self.data_to_iterate = self.get_image_data()\n",
    "        self.anomaly_source_paths = sorted(1 * glob.glob(anomaly_source_path + \"/*/*.jpg\") +\n",
    "                                           0 * list(self.imgpaths.values())[0])\n",
    "\n",
    "        self.transform_img = [\n",
    "            transforms.Resize(self.resize),\n",
    "            transforms.ColorJitter(brightness_factor, contrast_factor, saturation_factor),\n",
    "            transforms.RandomHorizontalFlip(h_flip_p),\n",
    "            transforms.RandomVerticalFlip(v_flip_p),\n",
    "            transforms.RandomGrayscale(gray_p),\n",
    "            transforms.RandomAffine(rotate_degrees,\n",
    "                                    translate=(translate, translate),\n",
    "                                    scale=(1.0 - scale, 1.0 + scale),\n",
    "                                    interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(self.imgsize),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "        ]\n",
    "        self.transform_img = transforms.Compose(self.transform_img)\n",
    "\n",
    "        self.transform_mask = [\n",
    "            transforms.Resize(self.resize),\n",
    "            transforms.CenterCrop(self.imgsize),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        self.transform_mask = transforms.Compose(self.transform_mask)\n",
    "\n",
    "        # image processing for speefox data\n",
    "        self.data_to_iterate_image_path = []\n",
    "        self.data_to_iterate_image = []\n",
    "        self.data_to_iterate_anomaly = []\n",
    "        for idx in range(len(self.data_to_iterate)):\n",
    "            anomaly, image_path = self.data_to_iterate[idx] # [anomaly유형, img경로]\n",
    "            image = PIL.Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            rows, cols = 3, 6\n",
    "            width, height = image.size\n",
    "\n",
    "            tile_width = width // cols\n",
    "            tile_height = height // rows\n",
    "\n",
    "            for col in range(cols):\n",
    "                left = col * tile_width\n",
    "                upper = tile_height\n",
    "                right = left + tile_width\n",
    "                lower = tile_height * 2\n",
    "                tile = image.crop((left, upper, right, lower))\n",
    "                tile = self.transform_img(tile)\n",
    "                \n",
    "                self.data_to_iterate_image_path.append(image_path)\n",
    "                self.data_to_iterate_image.append(tile)\n",
    "                self.data_to_iterate_anomaly.append(anomaly)\n",
    "\n",
    "        print(f\"len data per row: {len(self.data_to_iterate_image)} ({len(self.data_to_iterate)}*6)\")\n",
    "\n",
    "    def rand_augmenter(self):\n",
    "        list_aug = [\n",
    "            transforms.ColorJitter(contrast=(0.8, 1.2)),\n",
    "            transforms.ColorJitter(brightness=(0.8, 1.2)),\n",
    "            transforms.ColorJitter(saturation=(0.8, 1.2), hue=(-0.2, 0.2)),\n",
    "            transforms.RandomHorizontalFlip(p=1),\n",
    "            transforms.RandomVerticalFlip(p=1),\n",
    "            transforms.RandomGrayscale(p=1),\n",
    "            transforms.RandomAutocontrast(p=1),\n",
    "            transforms.RandomEqualize(p=1),\n",
    "            transforms.RandomAffine(degrees=(-45, 45)),\n",
    "        ]\n",
    "        aug_idx = np.random.choice(np.arange(len(list_aug)), 3, replace=False)\n",
    "\n",
    "        transform_aug = [\n",
    "            transforms.Resize(self.resize),\n",
    "            list_aug[aug_idx[0]],\n",
    "            list_aug[aug_idx[1]],\n",
    "            list_aug[aug_idx[2]],\n",
    "            transforms.CenterCrop(self.imgsize),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "        ]\n",
    "\n",
    "        transform_aug = transforms.Compose(transform_aug)\n",
    "        return transform_aug\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data_to_iterate_image_path[idx]\n",
    "        image = self.data_to_iterate_image[idx]\n",
    "        anomaly = self.data_to_iterate_anomaly[idx]\n",
    "        \n",
    "        mask_fg = mask_s = aug_image = torch.tensor([1])\n",
    "        if self.split == DatasetSplit.TRAIN:\n",
    "            aug = PIL.Image.open(np.random.choice(self.anomaly_source_paths)).convert(\"RGB\")\n",
    "            if self.rand_aug:\n",
    "                transform_aug = self.rand_augmenter()\n",
    "                aug = transform_aug(aug)\n",
    "            else:\n",
    "                aug = self.transform_img(aug)\n",
    "\n",
    "            mask_all = perlin_mask(image.shape, self.imgsize // self.downsampling, 0, 6, mask_fg, 1)\n",
    "            mask_s = torch.from_numpy(mask_all[0])\n",
    "            mask_l = torch.from_numpy(mask_all[1])\n",
    "\n",
    "            beta = np.random.normal(loc=self.mean, scale=self.std)\n",
    "            beta = np.clip(beta, .2, .8)\n",
    "            aug_image = image * (1 - mask_l) + (1 - beta) * aug * mask_l + beta * image * mask_l\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"aug\": aug_image,\n",
    "            \"mask_s\": mask_s,\n",
    "            \"is_anomaly\": int(anomaly != \"good\"),\n",
    "            \"image_path\": image_path,\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_to_iterate * 5)\n",
    "    \n",
    "    def get_image_data(self):\n",
    "        imgpaths = {}\n",
    "\n",
    "        classpath = os.path.join(self.source, self.split.value) # e.g. D:/Dev/Data/mvtec/speefox/test\n",
    "        if not self.anomaly_type:\n",
    "            anomaly_types = os.listdir(classpath) # [\"good\", \"A\", \"AI\", \"B\", \"E\", \"P\", \"T\", \"U\", \"V\", \"W\"]\n",
    "            for anomaly in anomaly_types:\n",
    "                anomaly_path = os.path.join(classpath, anomaly) # e.g. D:/Dev/Data/mvtec/speefox/test/A\n",
    "                anomaly_files = sorted(os.listdir(anomaly_path))\n",
    "                imgpaths[anomaly] = [os.path.join(anomaly_path, x) for x in anomaly_files]\n",
    "        else: # anomaly_type이 명시된 경우\n",
    "            anomaly_path = os.path.join(classpath, self.anomaly_type)\n",
    "            anomaly_files = sorted(os.listdir(anomaly_path))\n",
    "            imgpaths[self.anomaly_type] = [os.path.join(anomaly_path, x) for x in anomaly_files]\n",
    "\n",
    "\n",
    "        data_to_iterate = []\n",
    "        for anomaly in sorted(imgpaths.keys()):\n",
    "            for i, image_path in enumerate(imgpaths[anomaly]):\n",
    "                data_tuple = [anomaly, image_path]\n",
    "                data_to_iterate.append(data_tuple)\n",
    "        return imgpaths, data_to_iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.set_torch_device(gpu_ids=args.device)\n",
    "model = glass.GLASS(device)\n",
    "\n",
    "backbone = backbones.load(args.backbone)\n",
    "backbone.name, backbone.seed = args.backbone, None\n",
    "\n",
    "model.load(\n",
    "    backbone                 = backbone,\n",
    "    layers_to_extract_from   = args.layers_to_extract_from,\n",
    "    device                   = device,\n",
    "    input_shape              = (3, image_size, image_size), ##\n",
    "    pretrain_embed_dimension = args.pretrain_embed_dimension,\n",
    "    target_embed_dimension   = args.target_embed_dimension,\n",
    "    patchsize                = args.patchsize,\n",
    "    meta_epochs              = 640,\n",
    "    eval_epochs              = 1,\n",
    "    dsc_layers               = 4,\n",
    "    dsc_hidden               = 1024,\n",
    "    dsc_margin               = 0.5,\n",
    ")\n",
    "\n",
    "# 저장 경로 설정\n",
    "save_path = \"saved\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "models_dir = os.path.join(save_path, \"models\")\n",
    "model.set_model_dir(os.path.join(models_dir, f\"backbone\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "anomaly_type: [\"good\", \"A\", \"AI\", \"B\", \"E\", \"P\", \"T\", \"U\", \"V\", \"W\"]\n",
    "\"\"\"\n",
    "\n",
    "# normal data\n",
    "test_dataset = AnomalyDataset(\n",
    "    source=\"D:/Dev/Data/speefox\",\n",
    "    anomaly_source_path=\"D:/Dev/Data/dtd/images\",\n",
    "    resize=image_size,\n",
    "    imagesize=image_size,\n",
    "    split=DatasetSplit.TEST,\n",
    "    anomaly_type=\"good\"\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=6,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    # prefetch_factor=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# images_per_cap_normal, scores_per_cap_normal, masks_per_cap_normal = model.tester_speefox(test_dataloader)\n",
    "scores_per_cap_normal = model.tester_speefox(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "anomaly_type: [\"good\", \"A\", \"AI\", \"B\", \"E\", \"P\", \"T\", \"U\", \"V\", \"W\"]\n",
    "\"\"\"\n",
    "\n",
    "# abnormal data\n",
    "test_dataset = AnomalyDataset(\n",
    "    source=\"D:/Dev/Data/speefox\",\n",
    "    anomaly_source_path=\"D:/Dev/Data/dtd/images\",\n",
    "    resize=image_size,\n",
    "    imagesize=image_size,\n",
    "    split=DatasetSplit.TEST,\n",
    "    anomaly_type=\"W\"\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=6,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    # prefetch_factor=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "scores_per_cap_abnormal = model.tester_speefox(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 8))\n",
    "plt.scatter(x=np.arange(len(scores_per_cap_normal)), y=scores_per_cap_normal, c=\"blue\")\n",
    "plt.scatter(x=np.arange(len(scores_per_cap_normal), len(scores_per_cap_normal)+len(scores_per_cap_abnormal)), y=scores_per_cap_abnormal, c=\"red\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
